# Data

This document describes the data layer in **bitcoin_ml_pipeline**: how raw market data is collected/loaded, how it is normalized into a tabular dataset, and what contracts downstream pipelines (features/training/backtest) can rely on.

## Scope

The data layer covers:

- Ingestion / collection (historical or real-time).
- Basic normalization (timestamps, column naming, indexing).
- Persisting raw and processed datasets to disk.

**Out of scope** (covered elsewhere):
- Feature engineering (`src/features`)
- Model training/inference (`src/models`, `src/pipelines`)
- Strategy/backtest logic (`src/strategy`, `src/backtest`)

## Data sources

### 1) CSV (local)
Used for reproducible runs and debugging. Typical flow:
- Raw CSV stored under `data/raw/`
- Loaded into a `pandas.DataFrame` and normalized

### 2) Yahoo Finance (yfinance)
Convenient for historical OHLCV downloads (e.g., `BTC-USD`).
Typical output columns:
- `Date`, `Open`, `High`, `Low`, `Close`, `Volume` (depending on the period)

### 3) Bitso (real-time / near-real-time quotes)
Used for live-ish polling (e.g., every 2 seconds).
This layer typically stores quote snapshots (bid/ask/mid, timestamp) in parquet partitions per run.

> **Terminology note:** In this project, `source` usually refers to the input type (e.g., `"csv"`), while `provider` refers to the external service configuration (e.g., Bitso client settings).

## Directory layout (convention)

Recommended layout (adjust to your repo if needed):

- `data/raw/`
  - Raw input files (CSV)
- `data/processed/`
  - Cleaned datasets ready for feature engineering
- `data/quotes`
    - Raw quote parquets
- `artifacts/`
  - Training outputs, models, evaluation reports (not raw/processed data)

> Raw/processed data can be treated as “artifacts” if they are generated by pipeline runs and are not meant to be hand-edited. For large datasets, consider git-ignoring them.

## Data contract

Downstream modules assume that the processed dataset follows a stable contract.

### Required columns (minimum)
- A time column (commonly `Date` or `timestamp`)
- A base price column used for feature computation (commonly `Close` or `mid`)
- A target column for supervised learning (commonly `LogReturn`)

### Timestamp requirements
- Stored in UTC (recommended), with timezone-aware timestamps where applicable.
- Monotonic increasing ordering (sorted by time).
- No duplicates (or deduplicated with a documented policy).

### Missing data policy
- Raw data may contain missing intervals (especially for polling collectors).
- Processed dataset should clearly define:
  - whether gaps are allowed
  - how missing values are handled (drop / forward-fill / reindex)
